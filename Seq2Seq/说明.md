1.Encoder-Decoder架构
编解码器（Encoder-Decoder）架构适用于输入和输出均为序列的任务，其由编码器和解码器两部分构成。编码器对输入序列编码，得到一个或一组表示向量C，该向量涵盖了输入序列的语义信息；解码器用表示向量C初始化解码出对应的目标序列。
2.Attention机制
注意力机制（Attention）是一种广泛应用于Encoder-Decoder架构的方法。Attention使得模型在解码的不同时刻可以有侧重地关注输入序列的不同部分，从而提升Seq2Seq任务的表现。
四、实验过程
本实验主要参考学长的E2EModel，并设计了带有一个头的自注意力机制的编码器和使用掩码的交叉注意力机制的解码器。使用自注意力机制的编码器实验效果一般，但是使用掩码注意力的解码器有更好的注意力解释性，并对模型有一些提升。
4.1、实验环境：
PyCharm	2023.1
CPU		intel core i7
GPU		GTX1650ti 内存4G
CUDA		12.1
torch		1.8.1+cu102
OS			windows11

4.2、项目目录：
项目目录如下图所示：

config.py存放项目配置
dataset.py用于构建数据集，进行数据加载和预处理
model.py用于搭建模型保存seq2seq、encoder、decoder、attention等模型
main.py是训练和验证的启动文件
test.py用于测试，生成结果代码
attention_visual.py用于生成注意力热力图
4.3、数据加载和预处理：
1、数据结构分析：
训练数据和验证数据结构：

包括mr和ref两列，每一行表示一个酒馆的描述，其中mr是描述该酒馆属性，ref是根据mr属性组成的描述该酒馆的通顺的话。其中，mr中所有属性共8类，并且按词频顺序分别是：name、food、priceRange、customer rating、familyFriendly、area、near、eatType。
2、数据预处理
有些属性较好判断，但是关于地名和店名的name、near就不是那么容易判别，如果将每一个name和near都分门别类，那么只会徒增词表长度，引入一些不必要的词，并且在生成阶段，判别这些词的好坏降低模型的准确率。在本实验中使用单词去重，将name全部转化为xname、near全部转化为xnear，但这并不意味着原来的词不需要了。本实验中会存储原数据mr中的这两个，以便在生成的时候直接将xname和xnear转化为原文的词汇，不仅减少工作量，也提高了模型准确率。
单词去重：
1.# 对mr数据进行预处理，补全属性、进行序列化  
2.mr_data = [PAD_TOKEN] * self.key_num  
3.lex = ['', '']  
4.for item in self.mr[index].items():  
5.    key = item[0]  
6.    value = item[1]  
7.    key_idx = self.field_tokenizer[key]  
8.    # 单词去重  
9.    if key == 'name':  
10.        mr_data[key_idx] = NAME_TOKEN  
11.        lex[0] = value  
12.    elif key == 'near':  
13.        mr_data[key_idx] = NEAR_TOKEN  
14.        lex[1] = value  
15.    else:  
16.        mr_data[key_idx] = value 
其中NAME_TOKEN和NEAR_TOKEN在config中：
1.# 数据相关  
2.self.PAD_TOKEN = '[PAD]'  
3.self.PAD_ID = 4  
4.self.BOS_ID = 1  
5.self.EOS_ID = 2  
6.self.UNK_ID = 3  
7.self.NAME_TOKEN = 'xname'  
8.self.NEAR_TOKEN = 'xnear' 

那么这个任务的目标就很明确，使用所给出的mr属性来“造句子”。mr共包含8个属性，但并不是所有样本8个属性都包含，为了保证数据维度相同，需要将没有的那部分属性用‘[PAD]’符号补全，使得所有样本的mr属性长度都为8，然后还需要保证属性对应的位置相同，那么就事先对所有的属性按词频排序，保持位置关系。具体的mr属性处理代码如下：
1.mr_key = list(map(lambda x: list(x.keys()), self.mr))  # 获取所有的属性  
2.counter = Counter()  
3.for line in mr_key:    
4.    counter.update(line)    
5.# 按词频对属性进行排序，相当于位置编码  
6._tokens = [(token, count) for token, count in counter.items()]  
7._tokens = sorted(_tokens, key=lambda x: -x[1])  
8.# 获取词列表  
9._tokens = [token for token, count in _tokens]  
10.# 创建word2token  
11.self.field_tokenizer = dict(zip(_tokens, range(len(_tokens))))  
12.self.key_num = len(self.field_tokenizer)

ref都是一些英语短句子，可以按对英语句子的处理，先与mr的单词去重做法相同，将多变的地名、店名用xname和xnear替换并记录，然后根据标点和空格对英语句子分词，并统计所有出现过的词。代码如下：
1.# 对目标序列进行预处理  
2.ref_data = self.ref[index]  
3.if ref_data == '':  
4.    ref_data = ['']  
5.else:  
6.    if lex[0]:  
7.        ref_data = ref_data.replace(lex[0], NAME_TOKEN)  
8.    if lex[1]:  
9.        ref_data = ref_data.replace(lex[1], NEAR_TOKEN)  
10.    ref_data = list(map(lambda x: re.split(r"([.,!?\"':;)(])", x)[0],  
11.                        ref_data.split())) 

当mr和ref处理完毕，就可以用于构建token2id的转化字典了，方式如下：
1.# 统计词频  
2.counter = Counter()  
3.for line in self.raw_data_x:  
4.    counter.update(line)  
5.for line in self.raw_data_y:  
6.    counter.update(line)  
7.# 排序  
8._tokens = [(token, count) for token, count in counter.items()]  
9._tokens = sorted(_tokens, key=lambda x: -x[1])  
10.# 构建词列表  
11._tokens = ['[PAD]', '[BOS]', '[EOS]', '[UNK]'] + [token for token, count in _tokens]  
12.# 创建token2id  
13.token2id = dict(zip(_tokens, range(len(_tokens))))  
14.# 根据token2id构建tokenizer  
15.self.tokenizer = Tokenizer(token2id) 

构建E2EDataset类用于对原数据进行载入、token2id的转换字典的构建、补全等操作，类及初始化如下：
1.class E2EDataset(Dataset):  
2.    def __init__(self, path, mode='train', field_tokenizer=None, tokenizer=None, max_src_len=80, max_target_len=80):  
3.        super(E2EDataset, self).__init__()  
4.        self.mode = mode  
5.        self.max_src_len = max_src_len  
6.        self.max_target_len = max_target_len  
7.        # field_token  
8.        self.field_tokenizer = field_tokenizer  
9.        self.key_num = 0  
10.        self.tokenizer = tokenizer  
11.        # preprocess  
12.        self.raw_data_x = []  
13.        self.raw_data_y = []  
14.        self.lexicalizations = []  
15.        self.muti_data_y = {}  
16.        self.padding = 0  

训练集、验证集结构与测试集并不相同，测试集只有MR一列，没有ref。只需要预处理MR一列即可，ref用空字符串补全，相当于向解码器输入了空，需要解码生成句子。
3、序列转化
获取了原数据集的mr和ref分词列表，还需要将词转化成等长的序号序列。就需要token2id的转化和padding操作了。
token2id和id2token在Token类中分别以encode和decode来实现，依赖于token2id映射字典：
1.class Tokenizer:  
2.    def __init__(self, token2id):  
3.        self.token2id = token2id  
4.        self.id2token = {value: key for key, value in self.token2id.items()}  
5.        self.vocab_size = len(self.token2id)  
6.  
7.    def encode(self, words):  
8.        token_ids = [self.token2id['[BOS]']]  
9.        for word in words:  
10.            token_ids.append(self.token2id.get(word, self.token2id['[UNK]']))  
11.        token_ids.append(self.token2id['[EOS]'])  
12.        return token_ids  
13.  
14.    def decode(self, token_ids):  
15.        words = []  
16.        for token in token_ids:  
17.            word = self.id2token[token]  
18.            if word in ['[BOS]', '[EOS]', '[PAD]']:  
19.                continue  
20.            words.append(word)  
21.        return ' '.join(words)

截取或补全函数padding如下：
1.def seq_padding(self, data, max_len, padding=None):  
2.    """ 
3.    把数据补全或截取至max_len 
4.    :param data: 数据 
5.    :param max_len: 序列最大长度 
6.    :param padding: 需要补全的符号，default='[PAD]'=0 
7.    :return: 补全之后的序列 
8.    """  
9.    # 获取需要填充的数据  
10.if padding is None:  
11.    padding = self.tokenizer.token2id['[PAD]']  
12.self.padding = padding  
13.# 填充长度  
14.padding_len = max_len - len(data)  
15.# 填充  
16.if padding_len > 0:  
17.    outputs = data + [padding] * padding_len  
18.# 截断  
19.else:  
20.    outputs = data[:max_len]  
21.return outputs 

4.4、模型搭建：
1、Encoder：
Encoder的工作是将词嵌入后的序列进行编码。Encoder层较为简单，就一个线性层和一个激活层，所谓的编码其实就是特征提取，或者说将低维向量提高维度。Encoder会返回两个值，一个是激活层输出encoder_outputs，是一个seq_len * batchsize * encoder_output_dim大小的张量，另一个是一个句子所有token之和c，是一个1 * batchsize * encoder_output_dim大小的张量。结构如下：
1.class Encoder(nn.Module):  
2.    def __init__(self, input_size, hidden_size):  
3.        super(Encoder, self).__init__()  
4.        self.input_size = input_size  
5.        self.hidden_size = hidden_size  
6.        self.layer = nn.Linear(self.input_size, self.hidden_size)  
7.        self.relu = nn.ReLU()  
8.  
9.    def forward(self, x):  
10.    seq_len, batch_size, emb_dim = x.size()  
11.       outputs = self.relu(self.layer(x.view(-1, emb_dim)))  
12.    outputs = outputs.view(seq_len, batch_size, -1)  
13.    dec_hidden = torch.sum(outputs, 0)  
14.    return outputs, dec_hidden.unsqueeze(0)

2、Attention：
注意力实现单元，用于计算Q跟K的相关性α的，也就是代码中给出的attention_weights。输入是encoder的激活层输出和decoder的上一时刻隐藏层的输出，输出是注意力权重。
1.class Attention(nn.Module):  
2.    def __init__(self, encoder_dim, decoder_dim, attention_dim=None):  
3.        super(Attention, self).__init__()  
4.        self.num_directions = 1  
5.        self.h_dim = encoder_dim  
6.        self.s_dim = decoder_dim  
7.        self.a_dim = self.s_dim if attention_dim is None else attention_dim  
8.  
9.        self.U = nn.Linear(self.h_dim * self.num_directions, self.a_dim)  
10.        self.W = nn.Linear(self.s_dim, self.a_dim)  
11.        self.v = nn.Linear(self.a_dim, 1)  
12.        self.tanh = nn.Tanh()  
13.        self.softmax = nn.Softmax()  
14.  
15.    def forward(self, prev_h_batch, encoder_outputs):  
16.        src_seq_len, batch_size, enc_dim = encoder_outputs.size()  
17.        # uh -> seq_len * b * a_dim  
18.        uh = self.U(encoder_outputs.view(-1, self.h_dim)).view(src_seq_len, batch_size, self.a_dim)  
19.        # wq -> 1 * b * a_dim  
20.        wq = self.W(prev_h_batch.view(-1, self.s_dim)).unsqueeze(0)  
21.        wq3d = wq.expand_as(uh)  
22.        wquh = self.tanh(wq3d + uh)  
23.        attention_scores = self.v(wquh.view(-1, self.a_dim)).view(batch_size, src_seq_len)  
24.        attention_weights = self.softmax(attention_scores)  
25.        return attention_weights 

3、Decoder：
使用单向RNN单元GRU作为解码器，初始输入为encoder输出的token之和c、encoder的输出encoder_output、句子首部‘[BOS]’的词嵌入向量。输出是预测单词概率、GRU输出c_prime、注意力权重。之后的输入是上一时间步GRU输出c_prime、encoder的输出encoder_output、标签的词嵌入向量（该时间步应该输出的答案作为输入，使用教师指导的训练方式，收敛效果好）。
具体过程如下：
上一时间步隐藏层输出c_prime作为下一时间步输入与encoder输出计算注意力权重α。α表征了c_prime与输入序列的每个词的相关性。使用这个权重α为encoder的token之和输出也就是初始输入decoder的c进行加权求和，获得了具有上下文语义信息的新的c_prime，再将其与输入的教师指导的标签词嵌入向量进行拼接，就获得了标准答案在输入序列中、具有上下文语义信息的张量。再将这个张量放入线性层，进行特征提取，然后放入GRU，计算输出词的概率并为下一个时间步输入做准备。
1.class Decoder(nn.Module):  
2.    def __init__(self, input_size, hidden_size, output_size, embedding_dim, encoder_hidden_size):  
3.        super(Decoder, self).__init__()  
4.        self.rnn = nn.GRU(input_size, hidden_size, bidirectional=False)  
5.  
6.        self.attention_module = Attention(encoder_hidden_size, hidden_size)  
7.        self.W_combine = nn.Linear(embedding_dim + encoder_hidden_size, hidden_size)  
8.        self.W_out = nn.Linear(hidden_size, output_size)  
9.        self.log_softmax = nn.LogSoftmax()  
10.  
11.    def forward(self, prev_y_batch, prev_h_batch, encoder_outputs_batch):  
12.        # 上一时刻隐藏层输出q 与 编码器输出k 计算注意力相关系数alpha，输出大小 b * seq_len  
13.        attention_weights = self.attention_module(prev_h_batch, encoder_outputs_batch)  
14.        # 根据alpha对编码器输出进行加权，获得v  
15.        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs_batch.transpose(0, 1))  
16.        # 使用单向RNN进行解码，输入张量: b * (prev_y_dim + (enc_dim * num_enc_directions))  
17.        y_ctx = torch.cat((prev_y_batch, context.squeeze(1)), 1)  
18.        rnn_input = self.W_combine(y_ctx)  
19.  
20.        dec_rnn_output, dec_hidden = self.rnn(rnn_input.unsqueeze(0), prev_h_batch)  
21.        # 计算概率  
22.        unnormalized_logits = self.W_out(dec_rnn_output[0])  
23.        dec_output = self.log_softmax(unnormalized_logits)  
24.        # 返回最终输出、隐藏状态以及注意力权重  
25.        return dec_output, dec_hidden, attention_weights 

4、E2EModel：
整合了embedding、encoder、decoder，模型结构如下：
1.class E2EModel(nn.Module):  
2.    def __init__(self, src_vocab_size, target_vocab_size, cfg=config):  
3.        super(E2EModel, self).__init__()  
4.        self.device = cfg.device  
5.        self.cfg = cfg  
6.        self.src_vocab_size = src_vocab_size  
7.        self.target_vocab_size = target_vocab_size  
8.        # 词嵌入层  
9.        self.embedding_mat = nn.Embedding(src_vocab_size, cfg.embedding_dim, padding_idx=self.cfg.PAD_ID)  
10.        self.embedding_dropout_layer = nn.Dropout(self.cfg.embedding_dropout)  
11.        self.encoder = Encoder(input_size=self.cfg.encoder_input_dim,  
12.                               hidden_size=self.cfg.encoder_output_dim)  
13.    self.decoder = Decoder(input_size=self.cfg.decoder_input_dim,  
14.                               hidden_size=self.cfg.decoder_output_dim,  
15.                               output_size=self.target_vocab_size,  
16.                               embedding_dim=self.cfg.embedding_dim,  
17.                               encoder_hidden_size=self.cfg.encoder_output_dim) 

用于训练的前向传播forward如下：
1.def forward(self, data):  
2.    batch_x, batch_y = data  # source_len * b, target_len * b  
3.    # 词嵌入 seq_len * b * emd_dim  
4.    encoder_input_embedded = self.embedding_mat(batch_x)  
5.    encoder_input_embedded = self.embedding_dropout_layer(encoder_input_embedded)  
6.    # encode  
7.    encoder_outputs, encoder_hidden = self.encoder(encoder_input_embedded)  
8.    # decode  
9.    dec_len = batch_y.size()[0]  
10.    batch_size = batch_y.size()[1]  
11.    dec_hidden = encoder_hidden  
12.    # 解码器的初始输入，全BOS输入  
13.    dec_input = Variable(torch.LongTensor([self.cfg.BOS_ID] * batch_size)).to(self.device)  
14.    # 解码器输出概率，初始化为全零  
15.    logits = Variable(torch.zeros(dec_len, batch_size, self.target_vocab_size)).to(self.device)  
16.  
17.    # teacher forcing  
18.    for di in range(dec_len):  
19.        # 上一输出的词嵌入 b * emb_dim  
20.        prev_y = self.embedding_mat(dec_input)  
21.        # 解码  
22.        dec_output, dec_hidden, attention_weights = self.decoder(prev_y, dec_hidden, encoder_outputs)  
23.        logits[di] = dec_output  # 记录输出词概率  
24.        dec_input = batch_y[di]  # teacher forcing，下一词为标准答案  
25.  
26.    return logits  # size: dec_len * batch_size * target_vocab_size 

用于生成预测的predict如下：
1.def predict(self, input_var):  
2.    # embedding  
3.    encoder_input_embedded = self.embedding_mat(input_var)  
4.    # encode  
5.    encoder_outputs, encoder_hidden = self.encoder(encoder_input_embedded)  
6.    # decode  
7.    dec_ids, attention_w = [], []  
8.    curr_token_id = self.cfg.BOS_ID  
9.    curr_dec_idx = 0  # 记录生成的词数  
10.    dec_input_var = Variable(torch.LongTensor([curr_token_id]))  
11.    dec_input_var = dec_input_var.to(self.device)  
12.    dec_hidden = encoder_hidden[:1]  # 1 * b * encoder_dim  
13.    # 生成结束条件为生成EOS或者达到max_target_len  
14.    while curr_token_id != self.cfg.EOS_ID and curr_dec_idx <= self.cfg.max_target_len:  
15.        prev_y = self.embedding_mat(dec_input_var)  # 上一输出的词嵌入,size:b * emb_dim  
16.        # decode  
17.        decoder_output, dec_hidden, decoder_attention = self.decoder(prev_y, dec_hidden, encoder_outputs)  
18.        # 记录注意力alpha  
19.        attention_w.append(decoder_attention.data.cpu().numpy().tolist()[0])  
20.        # 选择最大概率的词作为下一个状态的输入  
21.        topval, topidx = decoder_output.data.topk(1)  
22.        curr_token_id = topidx[0][0]  
23.        # 记录解码结果  
24.        dec_ids.append(int(curr_token_id.cpu().numpy()))  
25.        # 进行下一轮生成  
26.        dec_input_var = (Variable(torch.LongTensor([curr_token_id]))).to(self.device)  
27.        curr_dec_idx += 1  
28.    return dec_ids, attention_w 

还会返回注意力矩阵，用于分析注意力相关性，画出热力图。

4.5、评价标准BLEU：
构建BLEUScore类来计算验证集上的得分，用来评估模型的好坏。初始化和构造函数如下：
1.class BLEUScore:  
2.    TINY = 1e-15  
3.    SMALL = 1e-9  
4.  
5.    def __init__(self, max_ngram=4, case_sensitive=False):  
6.        self.max_ngram = max_ngram  # 最大 n-gram  
7.        self.case_sensitive = case_sensitive  
8.        self.ref_len = 0  
9.        self.cand_lens = [0] * self.max_ngram  
10.        self.hits = [0] * self.max_ngram

1、类重置：
每轮验证都互不相干，但又不能声明很多个计算得分类，需要重置函数来清空上一次验证的所有信息，使得分数计算器回到初始化阶段。
1.def reset(self):  
2.    self.ref_len = 0  
3.    self.cand_lens = [0] * self.max_ngram  
4.    self.hits = [0] * self.max_ngram  

2、得分计算：
就是下图的一个翻译：

1.def score(self):  
2.    bp = 1.0  
3.    # c <= r : BP=e^(1-r/c)  
4.    # c > r : BP=1.0  
5.    if self.cand_lens[0] <= self.ref_len:  
6.        bp = math.exp(1.0 - self.ref_len / (float(self.cand_lens[0])  
7.                                            if self.cand_lens[0] else 1e-5))  
8.    prec_log_sum = 0.0  
9.    for n_hits, n_len in zip(self.hits, self.cand_lens):  
10.        n_hits = max(float(n_hits), self.TINY)  
11.  
12.        n_len = max(float(n_len), self.SMALL)  
13.        # 计算∑logPn=∑log(n_hits/n_len)  
14.        prec_log_sum += math.log(n_hits / n_len)  
15.    return bp * math.exp((1.0 / self.max_ngram) * prec_log_sum)

4.6、训练和验证：
1、数据集、迭代器构建：
1.# 训练数据集加载  
2.train_dataset = E2EDataset(path=cfg.train_dataset_path,  
3.                           mode='train',  
4.                           max_src_len=cfg.max_src_len,  
5.                           max_target_len=cfg.max_target_len)  
6.# 验证数据集加载  
7.dev_dataset = E2EDataset(path=cfg.dev_dataset_path,  
8.                         mode='dev',  
9.                         max_src_len=cfg.max_src_len,  
10.                         max_target_len=cfg.max_target_len,  
11.                         field_tokenizer=train_dataset.field_tokenizer,  
12.                         tokenizer=train_dataset.tokenizer)  
13.  
14.# 定义数据加载器  
15.train_loader = DataLoader(dataset=train_dataset,  
16.                          batch_size=cfg.batch_size)  
17.dev_loader = DataLoader(dataset=dev_dataset,  
18.                        batch_size=1)

3、模型、损失函数、优化器、验证分数计算器构建：
model				E2EModel
loss function			NLLLoss
optimizer			SGD
1.# 定义模型  
2.model = E2EModel(src_vocab_size=train_dataset.tokenizer.vocab_size,  
3.                 target_vocab_size=train_dataset.tokenizer.vocab_size,  
4.                 cfg=cfg).to(device)  
5.  
6.# 损失函数  
7.weight = torch.ones(train_dataset.tokenizer.vocab_size)  
8.weight[cfg.PAD_ID] = 0  
9.loss_function = nn.NLLLoss(weight=weight, size_average=True).to(device)  
10.  
11.# 优化器  
12.optimizer = optim.SGD(params=model.parameters(), lr=cfg.lr)  
13.  
14.# 学习率随着迭代次数减少 -> 收敛更快  
15.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)  
16.  
17.# 声明分数计算器  
18.scorer = BLEUScore(max_ngram=4)

4、训练：
主要的训练过程在模型构建的时候已经分析了，最核心的思想就是teacher forcing的训练过程，可以让训练的模型更稳定、收敛的更快更好。
过程就是将数据和标签投入到model中，model会返回预测的每个位置是词表中某个词的概率，生成的句子长度与标签相同，也就是max_seq_len。返回值logits是一个seq_len * batch_size * vocab_size的张量，使用NLLLoss损失函数可以直接计算损失，再用优化器优化参数即可。
1.def train(model, data_iter, iter):  
2.    print_loss = 0.0  
3.    model.train()  
4.    with tqdm.tqdm(total=len(data_iter), desc='epoch{} [train]'.format(iter), file=sys.stdout) as t:  
5.        for i, batch in enumerate(data_iter):  
6.            if i >= 163:  
7.                continue  
8.            source, target = batch  
9.            source = source.to(device).transpose(0, 1)  
10.            target = target.to(device).transpose(0, 1)  
11.            # 梯度清零  
12.            optimizer.zero_grad()  
13.            # 前向传播  
14.            logits = model((source, target))  
15.            vocab_size = logits.size()[-1]  
16.            logits = logits.contiguous().view(-1, vocab_size)  
17.            targets = target.contiguous().view(-1, 1).squeeze(1)  
18.            loss = loss_function(logits, targets.long())  
19.            print_loss += loss.data.item()  
20.            # 反向传播  
21.            loss.backward()  
22.            # 更新参数  
23.            optimizer.step()  
24.            # 更新进度条  
25.            t.set_postfix(loss=print_loss / (i + 1), lr=scheduler.get_last_lr()[0])  
26.            t.update(1)  
27.        # 记录loss、学习率的变化，更新学习率  
28.        loss_list.append(print_loss / len(data_iter))  
29.        learning_rate_list.append(scheduler.get_last_lr()[0])  
30.        scheduler.step()  
31.  
32.        t.close()  
33.        # 每val_num轮进行一次验证  
34.        if epoch % cfg.val_num == 0:  
35.            evaluate(model, dev_dataset, epoch) 


5、验证：
每val_num次进行一次验证，用于判断训练的模型好坏和保存模型。生成损失变化图、BLEU得分变化图。验证的时候用的是model的predict方法，用于生成，不能继续进行教师指导的策略。
1.def evaluate(model, dev_iter, iter):  
2.    global best_bleu  
3.    model.eval()  
4.    bleu = 0.0  
5.    total_num = 0  
6.    # 将分数计算器重置  
7.    scorer.reset()  
8.    with torch.no_grad():  
9.        with tqdm.tqdm(total=len(dev_iter), desc='[dev]'.format(iter), file=sys.stdout) as t:  
10.            for data in dev_iter:  
11.                src, tgt, lex, muti_tgt = data  
12.                src = torch.as_tensor(src[:, np.newaxis]).to(device)  
13.                sentence, attention = model.predict(src)  
14.                # decode  
15.                sentence = train_dataset.tokenizer.decode(sentence).replace('xname', lex[0]).replace('xnear', lex[1])  
16.                scorer.append(sentence, muti_tgt)  
17.                t.update(1)  
18.            t.close()  
19.    bleu = scorer.score()  
20.    bleu_list.append(bleu)  
21.    print('BLEU SCORE: {:.4f}'.format(bleu))  
22.    if bleu > best_bleu:  
23.        best_bleu = bleu  
24.        torch.save(model, 'my_best_model.pkl')  
25.        draw_carve('valid_bleu', 'valid_bleu.png', epoch//cfg.val_num + 1, bleu_list)  
26.    draw_carve('train_loss', 'train_loss.png', epoch + 1, loss_list)  
27.    draw_carve('train_lr', 'train_lr.png', epoch + 1, learning_rate_list)

4.7、测试和结果可视化分析：
1、测试：
加载训练好的模型'my_best_model.pkl'，使用该模型进行预测并生成result.txt文件。
1.# 开始测试  
2.model.eval()  
3.with torch.no_grad():  
4.    with tqdm.tqdm(total=len(test_dataset), desc='[test]'.format(iter), file=sys.stdout) as t:  
5.        for data in test_dataset:  
6.            src, tgt, lex, _ = data[0], data[1], data[2], data[3]  
7.            src = torch.as_tensor(src[:, np.newaxis]).to(device)  
8.            # 模型预测  
9.            sentence, attention = model.predict(src)  
10.        # 解码句子  
11.            sentence = test_dataset.tokenizer.decode(sentence).replace('xname', lex[0]).replace('xnear', lex[1])  
12.        # 写入文本  
13.        with open('result.txt', 'a+', encoding='utf-8') as f:  
14.            f.write(sentence + '.\n')  
15.print('Finished Testing!') 

2、注意力机制可视化：
使用model的predict方法返回的注意力权重，画出注意力权重的热力图，能够反应注意力相关性的大小，为分析每个词的生成过程提供依据。
1.model = torch.load('./my_best_model.pkl').to(device)  
2.dataset = E2EDataset(cfg.train_dataset_path, mode='train')  
3.dataset.mode = 'dev'  
4.  
5.src, tgt, lex, _ = dataset[0]  
6.src = torch.as_tensor(src[np.newaxis, :]).to(device).transpose(0, 1)  
7.sentence, attention = model.predict(src)  
8.  
9.# 绘制热力图  
10.ax = sns.heatmap(np.array(attention)[:, :10] * 100, cmap='YlGnBu')  
11.# 设置坐标轴  
12.plt.yticks([i + 0.5 for i in range(len(sentence_txt))], labels=sentence_txt, rotation=360, fontsize=12)  
13.plt.xticks([i + 0.5 for i in range(len(src_txt))], labels=src_txt, fontsize=12)  
14.plt.show() 
